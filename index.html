<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models</title>
  <meta name="description" content="Project page for multi-robot task planning that integrates LLM-based task decomposition and on-site spatial knowledge for knowledge-aware allocation." />
  <link rel="stylesheet" href="assets/css/style.css" />

  <!-- Canonical / Robots -->
  <link rel="canonical" href="https://kentomurata0610.github.io/multi-robot-task-planning/">
  <meta name="robots" content="index,follow" />

  <!-- Open Graph / Twitter -->
  <meta property="og:type" content="website">
  <meta property="og:site_name" content="Multi-Robot Task Planning">
  <meta property="og:title" content="Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models">
  <meta property="og:description" content="LLM-driven decomposition + on-site knowledge for cooperative multi-object retrieval by multiple robots.">
  <meta property="og:url" content="https://kentomurata0610.github.io/multi-robot-task-planning/">
  <meta property="og:image" content="https://kentomurata0610.github.io/multi-robot-task-planning/assets/img/teaser.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models">
  <meta name="twitter:description" content="LLM-driven decomposition + knowledge-aware allocation for cooperative object retrieval.">
  <meta name="twitter:image" content="https://kentomurata0610.github.io/multi-robot-task-planning/assets/img/teaser.png">

  <!-- Minimal add-on styles -->
  <style>
    html{scroll-behavior:smooth}
    .container{max-width:1100px;margin:0 auto;padding:0 18px}
    .center{ text-align:center }
    .muted{ opacity:.75 }
    .caption{ color:#666 }
    .btn{display:inline-block;padding:.55em .9em;border:1px solid #ddd;border-radius:10px;text-decoration:none}
    .btn:hover{background:#fafafa}
    .btn.tiny{ padding:.35em .6em; font-size:.9rem }
    .badge{font-size:.8em;background:#eef;border:1px solid #ccd;border-radius:8px;padding:.1em .5em;margin-left:.25em}
    figure{margin:1rem 0}
    figcaption{color:#666;font-size:.95rem}
    .media-grid.two{display:grid;grid-template-columns:1fr;gap:16px}
    @media(min-width:900px){.media-grid.two{grid-template-columns:1fr 1fr}}

    /* Top nav */
    .topnav{position:sticky;top:0;z-index:50;background:rgba(255,255,255,.85);backdrop-filter:saturate(180%) blur(6px);border-bottom:1px solid #eee}
    .topnav .navwrap{display:flex;gap:14px;padding:10px 0;flex-wrap:wrap;justify-content:center}
    .topnav a{color:#111;font-weight:600;opacity:.85;text-decoration:none}
    .topnav a:hover,.topnav a.active{opacity:1}

    /* Hero (centered) */
    .hero{padding:36px 0 20px}
    .hero h1{ font-size:2.2rem; line-height:1.15; margin:0 0 .6rem; }
    @media(min-width:900px){ .hero h1{ font-size:2.6rem; } }

    /* Authors compact row */
    .smart-authors{
      display:inline-flex; flex-wrap:wrap; gap:.4rem .6rem; align-items:center;
      color:#2b2b2b; font-size:1.05rem; justify-content:center;
    }
    .smart-authors a{ color:inherit; text-decoration:none; border-bottom:1px dashed rgba(0,0,0,.25) }
    .smart-authors a:hover{ border-bottom-color:rgba(0,0,0,.55) }
    .smart-authors .sep{ opacity:.45; user-select:none }
    .smart-authors .nolink{ border-bottom:1px dotted transparent }
    .corresponding{ margin-left:.15em; font-weight:700 }

    /* Affiliations (collapsed by default) */
    .affiliations{margin:.5rem auto 0 auto; padding-left:1.2rem; text-align:left; max-width:900px}
    .hidden{ display:none }

    /* Overview figure */
    .overview-figure{ max-width:980px; margin: 10px auto; }
    .overview-figure img{ width:100%; height:auto; border-radius:12px; border:1px solid #eee }

    /* Video container */
    .video-container{position:relative;padding-bottom:56.25%;height:0}
    .video-container iframe{position:absolute;top:0;left:0;width:100%;height:100%;border-radius:12px;border:1px solid #eee}

    /* Dark mode */
    @media (prefers-color-scheme: dark){
      .topnav{background:rgba(12,16,22,.75);border-bottom-color:#203040}
      .video-container iframe{border-color:#203040;background:#0a0f14}
      .badge{background:#1d2433;border-color:#2e3b55}
    }
  </style>

  <!-- Structured data (trimmed) -->
  <script type="application/ld+json">
  {
    "@context":"https://schema.org",
    "@type":"ScholarlyArticle",
    "name":"Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models",
    "author":[
      {"@type":"Person","name":"Kento Murata"},
      {"@type":"Person","name":"Shoichi Hasegawa"},
      {"@type":"Person","name":"Tomochika Ishikawa"},
      {"@type":"Person","name":"Yoshinobu Hagiwara"},
      {"@type":"Person","name":"Akira Taniguchi"},
      {"@type":"Person","name":"Lotfi El Hafi"},
      {"@type":"Person","name":"Tadahiro Taniguchi"}
    ],
    "url":"https://kentomurata0610.github.io/multi-robot-task-planning/",
    "image":"https://kentomurata0610.github.io/multi-robot-task-planning/assets/img/teaser.png",
    "datePublished":"2025-09-18"
  }
  </script>
</head>
<body>
  <header class="hero">
    <div class="container center">
      <h1>Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models</h1>

      <!-- Authors (centered, with Scholar links where available) -->
      <p class="authors">
        <span class="smart-authors">
          <span class="nolink">Kento Murata<span class="corresponding" title="Corresponding author">‚Ä†</span><sup>1</sup></span>
          <span class="sep">¬∑</span>
          <a href="https://scholar.google.co.jp/citations?user=KPxSCJUAAAAJ&hl=ja&oi=ao" target="_blank" rel="noopener" aria-label="Shoichi Hasegawa on Google Scholar">
            Shoichi Hasegawa<sup>1</sup>
          </a>
          <span class="sep">¬∑</span>
          <span class="nolink">Tomochika Ishikawa<sup>1</sup></span>
          <span class="sep">¬∑</span>
          <a href="https://scholar.google.co.jp/citations?user=Y4qjYvMAAAAJ&hl=ja&oi=ao" target="_blank" rel="noopener" aria-label="Yoshinobu Hagiwara on Google Scholar">
            Yoshinobu Hagiwara<sup>3,4</sup>
          </a>
          <span class="sep">¬∑</span>
          <a href="https://scholar.google.co.jp/citations?user=jtB7J0AAAAAJ&hl=ja&oi=ao" target="_blank" rel="noopener" aria-label="Akira Taniguchi on Google Scholar">
            Akira Taniguchi<sup>2</sup>
          </a>
          <span class="sep">¬∑</span>
          <a href="https://scholar.google.co.jp/citations?hl=ja&user=tsm7qaQAAAAJ" target="_blank" rel="noopener" aria-label="Lotfi El Hafi on Google Scholar">
            Lotfi El Hafi<sup>4</sup>
          </a>
          <span class="sep">¬∑</span>
          <a href="https://scholar.google.co.jp/citations?user=dPOCLQEAAAAJ&hl=ja&oi=ao" target="_blank" rel="noopener" aria-label="Tadahiro Taniguchi on Google Scholar">
            Tadahiro Taniguchi<sup>4,5</sup>
          </a>
        </span>
      </p>

      <!-- Affiliations toggle -->
      <button class="btn tiny" id="toggle-affil" type="button" aria-expanded="false" aria-controls="affiliations">
        Show affiliations
      </button>

      <ol id="affiliations" class="affiliations hidden" aria-label="Author affiliations">
        <li><sup>1</sup> Graduate School of Information Science and Engineering, Ritsumeikan University, Osaka, Japan</li>
        <li><sup>2</sup> College of Information Science and Engineering, Ritsumeikan University, Osaka, Japan</li>
        <li><sup>3</sup> Faculty of Science and Engineering, Soka University, Tokyo, Japan</li>
        <li><sup>4</sup> Research Organization of Science and Technology, Ritsumeikan University, Shiga, Japan</li>
        <li><sup>5</sup> Graduate School of Informatics, Kyoto University, Kyoto, Japan</li>
      </ol>

      <!-- Links row -->
      <p class="links" style="margin-top:.8rem">
        <a class="btn" href="https://arxiv.org/abs/2509.12838" target="_blank" aria-label="Open arXiv paper">üìÑ Paper (arXiv)</a>
        <a class="btn" href="paper/AROB_ISBC_2026_Murata.pdf" target="_blank" aria-label="Open submission PDF">
          üìÑ Submission PDF <span class="badge" aria-label="under review">under review</span>
        </a>
        <a class="btn" href="https://youtu.be/LMoWAp_kPhk" target="_blank" aria-label="Open demo video">‚ñ∂Ô∏è Demo Video</a>
      </p>
    </div>
  </header>

  <!-- Sticky local navigation -->
  <nav class="topnav" aria-label="Section navigation">
    <div class="container navwrap">
      <a href="#abstract">Abstract</a>
      <a href="#overview">Overview</a>
      <a href="#method">Method</a>
      <a href="#prompts">Prompts</a>  
      <a href="#experiments">Experiments</a>
      <a href="#resources">Resources</a>
      <a href="#demo">Demo</a>
      <a href="#bibtex">BibTeX</a>
      <a href="#ack">Acknowledgments</a>
    </div>
  </nav>

  <main class="container">
    <!-- 1) ABSTRACT FIRST (centered) -->
    <section id="abstract" class="center">
      <h2>Abstract</h2>
      <p class="lead" style="max-width:900px;margin:0 auto">
        We study cooperative object search by multiple robots that receive natural-language instructions including multi-object or context-dependent goals (e.g., ‚Äúfind an apple and a banana‚Äù).
        Our framework integrates a large language model (LLM) with a spatial concept model that provides room names and room-wise object presence probabilities learned on each robot‚Äôs assigned area.
        With a tailored prompting strategy, the LLM infers required items from ambiguous commands, decomposes them into subtasks, and allocates them to robots that are most likely to succeed given their local knowledge.
        In experiments, the method achieved <strong>47/50 successful allocations</strong>, outperforming random (28/50) and commonsense-only allocation (26/50), and was validated qualitatively on real mobile manipulators.
      </p>
    </section>

    <!-- 2) OVERVIEW (figure + concise explanation), then go to Method -->
    <section id="overview">
      <h2>Overview</h2>
      <figure class="overview-figure">
        <img src="assets/img/teaser.png"
             alt="Overview: language instructions are decomposed into subtasks and allocated to robots using on-site spatial knowledge."
             loading="lazy" decoding="async">
        <figcaption>Natural-language instructions ‚Üí decomposition ‚Üí knowledge-aware allocation ‚Üí execution.</figcaption>
      </figure>
      <p style="max-width:980px;margin:0 auto">
        Each robot maintains on-site knowledge that links places (e.g., kitchen, bedroom) with object‚Äìroom presence probabilities learned in its assigned area.
        Given a user instruction, an LLM performs task decomposition and assigns subtasks to robots expected to have higher success probabilities under local knowledge.
        Subtasks are executed by a skill sequence (<code>navigation ‚Üí object_detection ‚Üí pick ‚Üí place</code>) with feedback and replanning.
      </p>
    </section>
    <section id="method">
      <h2>Method</h2>
      <p>
        Each robot learns on-site knowledge via a spatial concept model that links places (e.g., kitchen, bedroom) and object occurrence probabilities.
        The pipeline has four stages: (1) task decomposition from language, (2) knowledge-aware subtask allocation,
        (3) sequential action planning (<code>navigation ‚Üí object_detection ‚Üí pick ‚Üí place</code>), and (4) execution with feedback loops (FlexBE).
      </p>
      <figure>
        <img src="assets/img/pipeline.png" alt="Four-stage pipeline: knowledge acquisition, decomposition & allocation, action planning, and feedback execution." loading="lazy" decoding="async">
        <figcaption>Four-stage pipeline: knowledge acquisition ‚Üí decomposition &amp; allocation ‚Üí action planning ‚Üí execution with feedback.</figcaption>
      </figure>
      <ul>
        <li><strong>On-site knowledge</strong>: place vocabulary + object‚Äìroom presence probabilities.</li>
        <li><strong>LLM prompts</strong>: few-shot designs to infer items, decompose tasks, and justify allocation using local probabilities.</li>
        <li><strong>Execution</strong>: closed-loop behaviors and replanning on success/failure signals.</li>
      </ul>
    </section>

    <section id="prompts">
      <h2>Prompts &amp; Configuration Files</h2>
      <p>
        This section provides the exact LLM prompts used in our experiments.
        If you are reviewing the paper and want to inspect the prompts, please start here.
      </p>
      <ul>
        <li>
          <strong>Task decomposition prompt:</strong>
          <a href="prompts/Task_Decomposition.txt" target="_blank" rel="noopener">
            Task_Decomposition.txt
          </a>
        </li>
        <li>
          <strong>Coalition / allocation prompt:</strong>
          <a href="prompts/Coalition_Formation.txt" target="_blank" rel="noopener">
            Coalition_Formation.txt
          </a>
        </li>
        <li>
          <strong>All prompts &amp; configs (zip):</strong>
          <a href="prompts/prompts_and_configs.zip" target="_blank" rel="noopener">
            prompts_and_configs.zip
          </a>
          <span class="caption">(contains all prompt templates and configuration files)</span>
        </li>
      </ul>

    <details style="margin-top:.8rem">
      <summary style="cursor:pointer;font-weight:700">
        Example: Task Decomposition Prompt
      </summary>
      <p class="caption">
        We provide the full prompt as <code>Task_Decomposition.txt</code>. Below we show only the header part.
      </p>
      <pre><code>from skills import navigation, object_detection, pick, put
  
  List of object_name:
  [plate, bowl, pitcher_base, banana, apple, orange, cracker_box, pudding_box, chips_bag, coffee, muscat, fruits_juice, pig_doll, sheep_doll, penguin_doll, airplane_toy, car_toy, truck_toy, tooth_paste, towel, cup, treatments, sponge, bath_slipper]
  
  
  def decompose_task(task_description):
      # GENERAL TASK DECOMPOSITION
      # Decompose and parallelize subtasks wherever possible.
      # (full prompt: see prompts/Task_Decomposition.txt)
      </code></pre>
    </details>

    <details style="margin-top:.8rem">
      <summary style="cursor:pointer;font-weight:700">
        Example: Knowledge-aware Allocation Prompt
      </summary>
      <p class="caption">
        Full version is in <code>Coalition_Formation.txt</code>. We show here the core structure.
      </p>
      <pre><code>from skills import navigation, object_detection, pick, put
  
  # TASK ALLOCATION
  # Scenario: There are 2 robots available. The task should be
  # performed using the minimum number of robots necessary.
  
  robots = [
      {"name":"Robot1", "skills":["navigation","object_detection","pick","put"],
       "found objects":["car_toy, bath_slipper"]},
      {"name":"Robot2", "skills":["navigation","object_detection","pick","put"],
       "found objects":["banana, apple, plate, plate, bowl, coffee"]}
  ]
  
  List of probabilities that an object exists at [living_room, kitchen, bedroom, bathroom]:
      plate: [0.005, 0.983, 0.007, 0.005]
      bowl:  [0.005, 0.983, 0.007, 0.005]
      ...
  # IMPORTANT: Think step by step and output only the final allocation.
      </code></pre>
    </details>


    <section id="experiments">
      <h2>Experiments</h2>
      <div class="media-grid two">
        <figure>
          <img src="assets/img/1_floor_env.png" alt="Evaluation environment ‚Äì first floor" loading="lazy" decoding="async">
          <figcaption>Evaluation environment ‚Äì First floor (5 rooms, 12 objects).</figcaption>
        </figure>
        <figure>
          <img src="assets/img/2_floor_env.png" alt="Evaluation environment ‚Äì second floor" loading="lazy" decoding="async">
          <figcaption>Evaluation environment ‚Äì Second floor (5 rooms, 12 objects).</figcaption>
        </figure>
      </div>
      <p>
        We evaluate allocation accuracy across instruction types (random, hard-to-predict, commonsense, mixed).
        The proposed method reaches <strong>47/50</strong> correct allocations versus <em>random</em> <strong>28/50</strong>
        and <em>commonsense-only</em> <strong>26/50</strong>.
      </p>
      <figure>
        <img src="assets/img/results.png" alt="Bar chart of allocation success counts across instruction types." loading="lazy" decoding="async">
        <figcaption>Allocation success counts across instruction types (random/hard/commonsense/mixed).</figcaption>
      </figure>
    </section>

    <section id="supplement">
      <h2>Supplement (Beyond the Paper)</h2>
      <ul>
        <li>
          <strong>Prompt skeletons</strong>:
          see the <a href="#prompts">Prompts &amp; Configuration Files</a> section.
        </li>
        <li><strong>Failure case</strong>: ...</li>
        <li><strong>Limitations</strong>: ...</li>
      </ul>
    </section>

    <section id="resources">
      <h2>Resources</h2>
      <ul>
        <li>Paper (arXiv): <a href="https://arxiv.org/abs/2509.12838" target="_blank" rel="noopener">arXiv:2509.12838</a></li>
        <li>Submission PDF (AROB/ISBC 2026, under review): <a href="paper/AROB_ISBC_2026_Murata.pdf" target="_blank" rel="noopener">PDF</a></li>
        <li>Video: <a href="https://youtu.be/LMoWAp_kPhk" target="_blank" rel="noopener">Demo</a></li>
      </ul>
      <p class="caption"><strong>Code availability:</strong> not publicly released at this time.</p>
    </section>

    <section id="demo">
      <h2>Demo Video</h2>
      <div class="video-container">
        <iframe
          src="https://www.youtube.com/embed/LMoWAp_kPhk"
          title="Demo video"
          frameborder="0"
          loading="lazy"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
          allowfullscreen>
        </iframe>
      </div>
      <p class="caption">Demonstration of our multi-robot task planning framework.</p>
    </section>

    <section id="bibtex">
      <h2>BibTeX</h2>
      <button class="btn" id="copyBib" type="button" aria-label="Copy BibTeX">Copy BibTeX</button>
      <pre class="bibtex" id="bibtexBlock">@article{Murata2025MultiRobotTaskPlanning,
  title   = {Multi-Robot Task Planning for Multi-Object Retrieval Tasks with Distributed On-Site Knowledge via Large Language Models},
  author  = {Murata, Kento and Hasegawa, Shoichi and Ishikawa, Tomochika and Hagiwara, Yoshinobu and Taniguchi, Akira and El Hafi, Lotfi and Taniguchi, Tadahiro},
  journal = {arXiv preprint arXiv:2509.12838},
  year    = {2025},
  note    = {Project page: https://kentomurata0610.github.io/multi-robot-task-planning/}
}
</pre>
      <p class="caption">Please cite the arXiv version until the conference review is complete.</p>
    </section>

    <section id="ack">
      <h2>Acknowledgments</h2>
      <p>
        Partially supported by JST Moonshot (JPMJMS2011), JSPS KAKENHI (JP25K15292, JP23K16975), and JST Challenging Research Program for Next-Generation Researchers (JPMJSP2101).
      </p>
    </section>
  </main>

  <footer class="footer">
    <div class="container">
      <p>¬© <span id="year"></span> The Authors. This page summarizes findings from the paper; please cite the paper if you use any part of this work.</p>
      <!-- Âõ∫ÂÆö„Åß2025„Å´„Åó„Åü„ÅÑÂ†¥Âêà„ÅØ„ÄÅ‰∏ä„ÅÆÂπ¥Ë°®Á§∫„Çí„Äå2025„Äç„Å´ÁΩÆ„ÅçÊèõ„Åà„Å¶‰∏ã„ÅÆ„Çπ„ÇØ„É™„Éó„Éà„ÇíÂâäÈô§„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ -->
    </div>
  </footer>

  <!-- Copy-to-clipboard for BibTeX -->
  <script>
    (function () {
      var btn = document.getElementById('copyBib');
      var pre = document.getElementById('bibtexBlock');
      if (!btn || !pre) return;
      btn.addEventListener('click', async function () {
        try {
          await navigator.clipboard.writeText(pre.textContent.trim());
          btn.textContent = 'Copied!';
          setTimeout(()=> btn.textContent = 'Copy BibTeX', 1600);
        } catch(e) { btn.textContent = 'Copy failed'; }
      });
    })();
  </script>

  <!-- Year auto-update -->
  <script>
    (function(){
      var y=document.getElementById('year');
      if(y) y.textContent = new Date().getFullYear();
    })();
  </script>

  <!-- ScrollSpy for topnav -->
  <script>
  (function(){
    const links = Array.from(document.querySelectorAll('.topnav a'));
    const map = new Map(links.map(a => [a.getAttribute('href'), a]));
    const opts = { rootMargin: '0px 0px -60% 0px', threshold: [0, 1.0] };
    const io = new IntersectionObserver(entries => {
      entries.forEach(e => {
        const id = '#' + e.target.id;
        const link = map.get(id);
        if (!link) return;
        if (e.isIntersecting) {
          links.forEach(l => l.classList.remove('active'));
          link.classList.add('active');
        }
      });
    }, opts);
    document.querySelectorAll('main section[id]').forEach(sec => io.observe(sec));
  })();
  </script>

  <script>
    (function(){
      const btn = document.getElementById('toggle-affil');
      const list = document.getElementById('affiliations');
      if(!btn || !list) return;
      btn.addEventListener('click', () => {
        const expanded = btn.getAttribute('aria-expanded') === 'true';
        btn.setAttribute('aria-expanded', String(!expanded));
        list.classList.toggle('hidden');
        btn.textContent = expanded ? 'Show affiliations' : 'Hide affiliations';
      });
    })();
  </script>
  
</body>
</html>
